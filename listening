{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T21:11:28.641503Z","iopub.execute_input":"2025-04-17T21:11:28.641976Z","iopub.status.idle":"2025-04-17T21:11:30.880830Z","shell.execute_reply.started":"2025-04-17T21:11:28.641944Z","shell.execute_reply":"2025-04-17T21:11:30.879838Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e4/sample_submission.csv\n/kaggle/input/playground-series-s5e4/train.csv\n/kaggle/input/playground-series-s5e4/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# %% [code]\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# %% [code]\n# Load data\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e4/sample_submission.csv')\n\n# Identify target column\ntarget_col = 'Listening_Time_minutes'\nprint(f\"Target variable: {target_col}\")\n\n# Drop id column\ntrain = train.drop('id', axis=1)\ntest_ids = test['id']\ntest = test.drop('id', axis=1)\n\n# %% [code]\n# Feature engineering\ndef create_features(df):\n    df = df.copy()\n    \n    # Convert categorical features\n    categorical_cols = ['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day']\n    for col in categorical_cols:\n        if col in df.columns:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str))\n    \n    # Convert Episode_Sentiment to numerical\n    if 'Episode_Sentiment' in df.columns:\n        sentiment_mapping = {'Positive': 1, 'Negative': -1, 'Neutral': 0}\n        df['Episode_Sentiment'] = df['Episode_Sentiment'].map(sentiment_mapping)\n        df['Episode_Sentiment'] = df['Episode_Sentiment'].fillna(0)  # Handle any missing\n    \n    # Handle Publication_Time\n    if 'Publication_Time' in df.columns:\n        try:\n            # Extract hour and minute\n            df['Publication_Hour'] = pd.to_datetime(df['Publication_Time']).dt.hour\n            df['Publication_Minute'] = pd.to_datetime(df['Publication_Time']).dt.minute\n        except:\n            # Default values if parsing fails\n            df['Publication_Hour'] = 12\n            df['Publication_Minute'] = 0\n    \n    # Create interaction features\n    df['Host_Guest_Interaction'] = df['Host_Popularity_percentage'] * df['Guest_Popularity_percentage']\n    df['Ads_Per_Minute'] = df['Number_of_Ads'] / (df['Episode_Length_minutes'] + 1)  # +1 to avoid division by zero\n    \n    return df\n\ntrain = create_features(train)\ntest = create_features(test)\n\n# Drop original time column if it exists\nif 'Publication_Time' in train.columns:\n    train = train.drop('Publication_Time', axis=1)\nif 'Publication_Time' in test.columns:\n    test = test.drop('Publication_Time', axis=1)\n\n# %% [code]\n# Define numerical columns for scaling (only truly numerical features)\nnumerical_cols = [\n    'Episode_Length_minutes',\n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage',\n    'Number_of_Ads',\n    'Publication_Hour',\n    'Publication_Minute',\n    'Host_Guest_Interaction',\n    'Ads_Per_Minute'\n]\n\n# Verify all numerical columns exist and are numeric\nfor col in numerical_cols:\n    if col not in train.columns:\n        numerical_cols.remove(col)\n    elif not pd.api.types.is_numeric_dtype(train[col]):\n        numerical_cols.remove(col)\n\nprint(\"Columns to scale:\", numerical_cols)\n\n# Scale numerical features\nscaler = RobustScaler()\ntrain[numerical_cols] = scaler.fit_transform(train[numerical_cols])\ntest[numerical_cols] = scaler.transform(test[numerical_cols])\n\n# %% [code]\n# Prepare data\nX = train.drop(target_col, axis=1)\ny = train[target_col]\nX_test = test.copy()\n\n# %% [code]\n# Model parameters\nxgb_params = {\n    'n_estimators': 1000,\n    'max_depth': 6,\n    'learning_rate': 0.01,\n    'subsample': 0.8,\n    'colsample_bytree': 0.6,\n    'gamma': 0.1,\n    'min_child_weight': 10,\n    'reg_alpha': 1,\n    'reg_lambda': 1,\n    'objective': 'reg:squarederror',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'random_state': 42,\n    'eval_metric': 'rmse'\n}\n\n# %% [code]\n# Cross-validation\nN_SPLITS = 5\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\ntest_preds = np.zeros(len(X_test))\nrmse_scores = []\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n    print(f\"\\nFold {fold + 1}\")\n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose=100\n    )\n    \n    valid_preds = model.predict(X_valid)\n    test_preds += model.predict(X_test) / N_SPLITS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_valid, valid_preds))\n    rmse_scores.append(fold_rmse)\n    print(f\"Fold {fold + 1} RMSE: {fold_rmse:.5f}\")\n\nprint(f\"\\nOverall RMSE: {np.mean(rmse_scores):.5f}\")\n\n# %% [code]\n# Create submission\nsubmission['Listening_Time_minutes'] = test_preds\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\nSubmission head:\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T21:15:24.656992Z","iopub.execute_input":"2025-04-17T21:15:24.657167Z","iopub.status.idle":"2025-04-17T21:16:15.454332Z","shell.execute_reply.started":"2025-04-17T21:15:24.657151Z","shell.execute_reply":"2025-04-17T21:16:15.453719Z"}},"outputs":[{"name":"stdout","text":"Target variable: Listening_Time_minutes\nColumns to scale: ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Publication_Hour', 'Publication_Minute', 'Host_Guest_Interaction', 'Ads_Per_Minute']\n\nFold 1\n[0]\tvalidation_0-rmse:26.98717\n[100]\tvalidation_0-rmse:17.79192\n[200]\tvalidation_0-rmse:14.52357\n[300]\tvalidation_0-rmse:13.58193\n[400]\tvalidation_0-rmse:13.27433\n[500]\tvalidation_0-rmse:13.15963\n[600]\tvalidation_0-rmse:13.11135\n[700]\tvalidation_0-rmse:13.08987\n[800]\tvalidation_0-rmse:13.07664\n[900]\tvalidation_0-rmse:13.06686\n[999]\tvalidation_0-rmse:13.05947\nFold 1 RMSE: 13.05947\n\nFold 2\n[0]\tvalidation_0-rmse:27.01822\n[100]\tvalidation_0-rmse:17.82729\n[200]\tvalidation_0-rmse:14.56502\n[300]\tvalidation_0-rmse:13.62765\n[400]\tvalidation_0-rmse:13.32267\n[500]\tvalidation_0-rmse:13.21004\n[600]\tvalidation_0-rmse:13.16227\n[700]\tvalidation_0-rmse:13.14077\n[800]\tvalidation_0-rmse:13.12778\n[900]\tvalidation_0-rmse:13.11753\n[999]\tvalidation_0-rmse:13.10995\nFold 2 RMSE: 13.10995\n\nFold 3\n[0]\tvalidation_0-rmse:26.98775\n[100]\tvalidation_0-rmse:17.82015\n[200]\tvalidation_0-rmse:14.56743\n[300]\tvalidation_0-rmse:13.63026\n[400]\tvalidation_0-rmse:13.32343\n[500]\tvalidation_0-rmse:13.20816\n[600]\tvalidation_0-rmse:13.15820\n[700]\tvalidation_0-rmse:13.13553\n[800]\tvalidation_0-rmse:13.12167\n[900]\tvalidation_0-rmse:13.11139\n[999]\tvalidation_0-rmse:13.10255\nFold 3 RMSE: 13.10255\n\nFold 4\n[0]\tvalidation_0-rmse:27.04858\n[100]\tvalidation_0-rmse:17.86110\n[200]\tvalidation_0-rmse:14.59032\n[300]\tvalidation_0-rmse:13.64126\n[400]\tvalidation_0-rmse:13.32761\n[500]\tvalidation_0-rmse:13.20816\n[600]\tvalidation_0-rmse:13.15600\n[700]\tvalidation_0-rmse:13.13154\n[800]\tvalidation_0-rmse:13.11722\n[900]\tvalidation_0-rmse:13.10618\n[999]\tvalidation_0-rmse:13.09769\nFold 4 RMSE: 13.09769\n\nFold 5\n[0]\tvalidation_0-rmse:26.95445\n[100]\tvalidation_0-rmse:17.76564\n[200]\tvalidation_0-rmse:14.51781\n[300]\tvalidation_0-rmse:13.58874\n[400]\tvalidation_0-rmse:13.28703\n[500]\tvalidation_0-rmse:13.17363\n[600]\tvalidation_0-rmse:13.12441\n[700]\tvalidation_0-rmse:13.10150\n[800]\tvalidation_0-rmse:13.08722\n[900]\tvalidation_0-rmse:13.07620\n[999]\tvalidation_0-rmse:13.06756\nFold 5 RMSE: 13.06756\n\nOverall RMSE: 13.08744\n\nSubmission head:\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"       id  Listening_Time_minutes\n0  750000               56.067822\n1  750001               18.111214\n2  750002               49.434804\n3  750003               79.639635\n4  750004               49.252007","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Listening_Time_minutes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>750000</td>\n      <td>56.067822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>750001</td>\n      <td>18.111214</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>750002</td>\n      <td>49.434804</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>750003</td>\n      <td>79.639635</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>750004</td>\n      <td>49.252007</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1}]}